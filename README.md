---

# ConfigurableMLPipeline

## ğŸ“Œ Overview  
A modular, configurable, and end-to-end machine learning pipeline for **regression tasks**, designed to automate data ingestion, preprocessing, training, evaluation, and hyperparameter tuning.   
> âš ï¸ Flask integration is manual â€” user must manually select a model and update `app.py`.

---

## ğŸ¯ Key Features
- Configurable via `config.json`
- Modular folder and file structure
- Supports 9 regression models
- Cross-validation and hyperparameter tuning
- Model performance logging and saving
- Manual Flask app for serving predictions
- Notebooks for EDA and experimentation

---

## ğŸ› ï¸ Tech Stack
- Python 3.9  
- Libraries: `scikit-learn`, `xgboost`, `pandas`, `numpy`, `joblib`, `Flask`, `matplotlib`, `seaborn`, `logging`

---

## ğŸ“‚ Project Structure
```

ML\_project/
â”œâ”€â”€ app.py                     # Flask app (manual integration)
â”œâ”€â”€ main.py                    # Entry point for pipeline
â”œâ”€â”€ config.json                # Configuration file
â”œâ”€â”€ requirements.txt           # Dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ coffee\_shop\_revenue.csv
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline\_log.logs
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ tuned models/
â”‚   â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb
â”‚   â””â”€â”€ experiments.ipynb
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ form.html
â”‚   â””â”€â”€ result.html
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config\_loader.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ load\_data.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ hyperparameter\_tuning.py
â”‚   â”‚   â”œâ”€â”€ model\_configurater.py
â”‚   â”‚   â””â”€â”€ model\_training.py
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ run\_pipeline.py
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ cross\_validation.py
â”‚   â”‚   â”œâ”€â”€ feature\_selection.py
â”‚   â”‚   â”œâ”€â”€ feature\_scaling.py
â”‚   â”‚   â””â”€â”€ splitting\_data.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger\_code.py

````

---

## âš™ï¸ How It Works

1. Load dataset from path specified in `config.json`
2. Preprocess data:
   - Split into train/test sets
   - Cross-validation score to get baseline metrics
   - Feature selection
   - Cross-validation with selected features
   - Feature scaling
   - Train models and evaluate them on test data
   - Select N number of models to hyper-tune
   - Hyper-parameter tune and log and save hyper-tuned models with their metrics and best parameters

3. Train 9 models using cross-validation
4. Select top N models for hyperparameter tuning
5. Save best model(s) and evaluation metrics
6. Use Jupyter notebooks for EDA and further analysis

---

## ğŸ§ª Supported Models
- Linear Regression  
- Ridge Regression  
- Lasso Regression  
- ElasticNet  
- Decision Tree  
- Random Forest  
- XGBoost  
- K-Nearest Neighbors  
- Support Vector Regressor

---

## ğŸ“Š Evaluation Metrics
- RMSE  
- MAE  
- RÂ²  
- MSE

Example (Random Forest):
- `RMSE = 227.21`  
- `RÂ² = 0.9475`  
- `MAE = 182.64`  
- `MSE = 51623.24`

---

## âš™ï¸ Configuration File (`config.json`) example.
```json
{
  "splitting": {
    "test_size": 0.2,
    "random_state": 3
  },
  "paths": {
    "outputs_directory": "outputs/",
    "dataset_path": "data/coffee_shop_revenue.csv"
  },
  "K_fold": {
    "n_splits": 5,
    "random_state_for_cv": 3
  },
  "feature_selection": {
    "selected_feature_selection_method": "feature_importance",
    "corr_threshold": 0.20,
    "no_of_features_to_select_using_feature_imp": 3,
    "features_to_select_k_using_mutual_info": 3
  },
  "scaling_method": {
    "scaler": "standard"
  },
  "no_of_models_to_select_for_hyperparameter_tuning": {
    "no_of_models": 2
  },
  "gridsearch_params": {
    "optimize_for": "neg_root_mean_squared_error",
    "n_jobs": -1,
    "cv": 5
  }
}
````

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone the repo
cd ConfigurableMLPipeline
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```
### 3. Update the paths in the config.json file and then update the path in the config_loader.py component to direct towards the config.json file

### 4. Run the Pipeline

```bash
python main.py
```

### 4. View Outputs

* Check `outputs/` for saved models and metrics
* Check `logs/pipeline_log.logs` for step-by-step logs

---

## ğŸŒ Flask App (Manual Integration)

1. Choose a model and scaler from `outputs/`
2. Update file paths manually in `app.py`
3. Run:

```bash
python app.py
```

4. Open browser at localhost

---

## ğŸ“Œ Notes

* Built to showcase modular pipeline design.
* Flask integration is **not** automated â€” this separation was intentional for flexibility and clarity.

---

## âœ… What I Learned

* End-to-end ML pipeline design
* Clean code structure and modularization
* Configuration-based architecture
* Manual vs. automated deployment trade-offs
* Logging, and output traceability

---
